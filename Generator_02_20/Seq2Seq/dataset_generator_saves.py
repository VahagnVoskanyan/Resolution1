import os
import json
import random
from clause_generator import ClauseGenerator, Term, Literal, Clause, ClausePair

class MGUDatasetCreator:
    def __init__(self, output_dir="data", random_seed=42):
        """
        Initialize the dataset creator.
        
        Args:
            output_dir: Directory to save the dataset files
            random_seed: Random seed for reproducibility
        """
        self.output_dir = output_dir
        random.seed(random_seed)
        self.generator = ClauseGenerator()
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def format_clause_pair(self, pair: ClausePair) -> dict:
        """
        Format a clause pair as a dictionary suitable for model training.
        
        Args:
            pair: A ClausePair object generated by ClauseGenerator
            
        Returns:
            Dictionary with formatted input and output strings
        """
        # Format the input: both clauses as strings
        input_str = f"Clause1: {str(pair.clause1)}\nClause2: {str(pair.clause2)}"
        
        # Format the complementary literals for additional context
        comp_lit1 = str(pair.clause1.literals[pair.complementary_lit_idx1])
        comp_lit2 = str(pair.clause2.literals[pair.complementary_lit_idx2])
        complementary_literals = f"{comp_lit1} and {comp_lit2}"
        
        # Format the output: MGU as a string
        mgu_str = "{" + ", ".join(f"{k}‚Üê{v}" for k, v in pair.mgu.items()) + "}"
        
        # Format the resolved clause
        resolved_str = str(pair.resolved_clause)
        
        return {
            "input": input_str,
            "complementary_literals": complementary_literals,
            "mgu": mgu_str,
            "resolved_clause": resolved_str,
            # Also include raw data for flexibility
            "clause1": str(pair.clause1),
            "clause2": str(pair.clause2),
            "mgu_raw": {k: str(v) for k, v in pair.mgu.items()}
        }
    
    def generate_dataset(self, train_size=8000, val_size=1000, test_size=1000):
        """
        Generate a dataset with train, validation, and test splits.
        
        Args:
            train_size: Number of examples in the training set
            val_size: Number of examples in the validation set
            test_size: Number of examples in the test set
        """
        # Generate the datasets
        print(f"Generating training set ({train_size} examples)...")
        train_pairs = self.generator.generate_dataset(train_size)
        
        print(f"Generating validation set ({val_size} examples)...")
        val_pairs = self.generator.generate_dataset(val_size)
        
        print(f"Generating test set ({test_size} examples)...")
        test_pairs = self.generator.generate_dataset(test_size)
        
        # Format the datasets
        print("Formatting training data...")
        train_data = []
        for i, pair in enumerate(train_pairs):
            train_data.append(self.format_clause_pair(pair))
            if (i + 1) % 1000 == 0:
                print(f"  Processed {i + 1}/{len(train_pairs)} examples")
        
        print("Formatting validation data...")
        val_data = []
        for i, pair in enumerate(val_pairs):
            val_data.append(self.format_clause_pair(pair))
            if (i + 1) % 500 == 0:
                print(f"  Processed {i + 1}/{len(val_pairs)} examples")
        
        print("Formatting test data...")
        test_data = []
        for i, pair in enumerate(test_pairs):
            test_data.append(self.format_clause_pair(pair))
            if (i + 1) % 500 == 0:
                print(f"  Processed {i + 1}/{len(test_pairs)} examples")
        
        # Save the datasets
        self.save_dataset(train_data, "train.jsonl")
        self.save_dataset(val_data, "val.jsonl")
        self.save_dataset(test_data, "test.jsonl")
        
        # Create a metadata file
        metadata = {
            "dataset_sizes": {
                "train": len(train_data),
                "val": len(val_data),
                "test": len(test_data)
            },
            "predicates": self.generator.predicates,
            "variables": self.generator.variables,
            "constants": self.generator.constants,
            "max_clause_length": self.generator.max_clause_length,
            "max_term_arity": self.generator.max_term_arity
        }
        
        with open(os.path.join(self.output_dir, "metadata.json"), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"Dataset generated and saved to {self.output_dir}/")
    
    def save_dataset(self, data, filename):
        """
        Save a dataset to a JSONL file.
        
        Args:
            data: List of formatted examples
            filename: Name of the output file
        """
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w') as f:
            for example in data:
                f.write(json.dumps(example) + '\n')
        print(f"Saved {len(data)} examples to {filepath}")
    
    def generate_variable_complexity_dataset(self, 
                                            sizes=[2000, 2000, 2000, 2000],
                                            clause_lengths=[2, 3, 4, 5], 
                                            term_arities=[2, 2, 3, 3]):
        """
        Generate datasets with varying complexity for curriculum learning.
        
        Args:
            sizes: List of dataset sizes for each complexity level
            clause_lengths: List of maximum clause lengths for each complexity level
            term_arities: List of maximum term arities for each complexity level
        """
        assert len(sizes) == len(clause_lengths) == len(term_arities), "Lists must have same length"
        
        for i, (size, max_length, max_arity) in enumerate(zip(sizes, clause_lengths, term_arities)):
            print(f"\nGenerating complexity level {i+1} dataset:")
            print(f"  - Max clause length: {max_length}")
            print(f"  - Max term arity: {max_arity}")
            print(f"  - Size: {size} examples")
            
            # Update generator parameters
            self.generator.max_clause_length = max_length
            self.generator.max_term_arity = max_arity
            
            # Generate dataset
            print(f"Generating {size} examples...")
            pairs = self.generator.generate_dataset(size)
            
            # Format dataset
            print("Formatting examples...")
            data = []
            for i, pair in enumerate(pairs):
                data.append(self.format_clause_pair(pair))
                if (i + 1) % 500 == 0:
                    print(f"  Processed {i + 1}/{len(pairs)} examples")
            
            # Save dataset
            self.save_dataset(data, f"level_{i+1}.jsonl")
        
        # Create a metadata file
        metadata = {
            "complexity_levels": [
                {
                    "level": i+1,
                    "size": size,
                    "max_clause_length": length,
                    "max_term_arity": arity
                }
                for i, (size, length, arity) in enumerate(zip(sizes, clause_lengths, term_arities))
            ],
            "predicates": self.generator.predicates,
            "variables": self.generator.variables,
            "constants": self.generator.constants
        }
        
        with open(os.path.join(self.output_dir, "complexity_metadata.json"), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\nComplexity datasets generated and saved to {self.output_dir}/")


def main():
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='Generate MGU dataset for sequence models')
    parser.add_argument('--output-dir', type=str, default='mgu_data', help='Output directory')
    parser.add_argument('--train-size', type=int, default=8000, help='Training set size')
    parser.add_argument('--val-size', type=int, default=1000, help='Validation set size')
    parser.add_argument('--test-size', type=int, default=1000, help='Test set size')
    parser.add_argument('--curriculum', action='store_true', help='Generate curriculum learning datasets')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # Create dataset creator
    creator = MGUDatasetCreator(output_dir=args.output_dir, random_seed=args.seed)
    
    # Generate dataset
    if args.curriculum:
        print("Generating curriculum learning datasets with varying complexity levels...")
        creator.generate_variable_complexity_dataset()
    else:
        print(f"Generating standard train/val/test split dataset...")
        creator.generate_dataset(
            train_size=args.train_size,
            val_size=args.val_size,
            test_size=args.test_size
        )
    
    print("Done!")


if __name__ == "__main__":
    main()